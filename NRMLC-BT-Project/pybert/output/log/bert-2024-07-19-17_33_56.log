Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='BERT-TEXTCNN', do_data=False, do_lower_case=True, do_test=True, do_train=False, epochs=6, eval_batch_size=4, eval_max_seq_len=128, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=1e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='0', predict_checkpoints=0, resume_path='', save_best=False, seed=42, sorted=1, train_batch_size=4, train_max_seq_len=128, valid_size=0.3, warmup_proportion=0.1, weight_decay=0.01)
Loading examples from cached file pybert/dataset/cached_test_examples_bert
Loading features from cached file pybert/dataset/cached_test_features_128_bert
loading configuration file pybert/output/checkpoints/bert/config.json
Model config BertConfig {
  "architectures": [
    "BertForMultiLable"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file pybert/output/checkpoints/bert/pytorch_model.bin
model predicting....
Label: Usa - AUC: 0.7745, Hamming Loss: 0.2620, Hamming Score: 0.2246, Precision: 0.8281, Recall: 0.7745, F1 Score: 0.7683
Label: Sup - AUC: 0.7800, Hamming Loss: 0.1892, Hamming Score: 0.0897, Precision: 0.9351, Recall: 0.7800, F1 Score: 0.7472
Label: Dep - AUC: 0.8027, Hamming Loss: 0.2139, Hamming Score: 0.1260, Precision: 0.8064, Recall: 0.8027, F1 Score: 0.7655
Label: Per - AUC: 0.9065, Hamming Loss: 0.1815, Hamming Score: 0.2142, Precision: 0.9277, Recall: 0.9065, F1 Score: 0.9103
              precision    recall  f1-score   support

         Usa       0.77      0.53      0.63       423
         Sup       0.76      0.30      0.43       183
         Dep       0.57      0.53      0.55       219
         Per       0.93      0.72      0.81       332

   micro avg       0.77      0.55      0.64      1157
   macro avg       0.76      0.52      0.60      1157
weighted avg       0.78      0.55      0.63      1157
 samples avg       0.46      0.41      0.43      1157


Test: test_auc: 0.8137 - test_precision: 0.7584 - test_recall: 0.5180 - test_hamming_score: 0.4665 - test_hamming_loss: 0.1718 - test_jac: 0.5157 - test_f1: 0.6699 
