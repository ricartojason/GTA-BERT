# GTA-BERT
一种使用改进的BERT模型进行非功能需求多标签分类的方法（A NFRs Multi Label Classification Method Using Enhanced Bert)

## 运行说明：
First need to run project in [Bert - base] (https://huggingface.co/google-bert/bert-base-uncased/tree/main) to download pytorch_model. Bin, config. Json, vocab.txt.json, vocab.txt.

- `bert-base-uncased-pytorch_model.bin` renamed to `pytorch_model.bin`
- `bert-base-uncased-config.json` renamed to `config.json`
- `bert-base-uncased-vocab.txt` renamed to `bert_vocab.txt`
Store this file in pretrain\bert\base-uncansed.

/configs/basic_config is used to change the model parameters and the path where the dataset is stored

/output/log is used to store model parameters and train/test results during model training

/output/figure is used to store the changes of evaluation indexes during training and validation

Run 'python run bert.py --do data' to process the data
Run 'python run_bert.py --do_train --save_best --do_lower_case' to train, validate, and fine-tune the model
Run 'run_bert.py --do_test --do_lower_case' to evaluate the performance on the test set

## 数据集说明：
The path to save the dataset is /dataset

review origin train.scv is the original training set

review origin Test.scv is the original test set

MNR_Data is the training set after integrating the new dataset, while MNR_Test is the test set after integrating the new dataset

review_single_table is the MNR-1 relabelled as a single-label dataset

In the Labelled datasets folder, we annotated 4600 reviews of 8 apps in the Quim Motger dataset, which we refer to in this paper as MNR-4

The datasets prefixed with TTA are all samples generated by GPT for Train/Test-Time augmentation (TTA)

To run TTA, set the is_augament parameter of the data.read_data() function to True in run_bert.py
